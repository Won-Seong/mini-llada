import os
import yaml
import argparse
import torch
from transformers import (
    Trainer, 
    TrainingArguments, 
    AutoTokenizer, 
    DataCollatorWithPadding
)
from datasets import load_dataset

# ì‚¬ìš©ìë‹˜ì´ ë§Œë“œì‹  ëª¨ë“ˆ ì„í¬íŠ¸
from ko_mini_llada.models.configuration_ko_mini_llada import LladaConfig
from ko_mini_llada.models.modeling_ko_mini_llada import KoMiniLlada
from ko_mini_llada.data.dataset import prepare_dataset

# (ì„ íƒ) ì´ì „ì— ë§Œë“  ìƒì„± í‰ê°€ ì½œë°±ì´ ìˆë‹¤ë©´ ì„í¬íŠ¸
from ko_mini_llada.utils.callbacks import GenerateSampleCallback 

def main():
    parser = argparse.ArgumentParser()
    parser.add_argument("--config_file", type=str, default="configs/config.yaml")
    parser.add_argument("--output_dir", type=str, default="checkpoints")
    parser.add_argument("--mode", type=str, default="pretrain", choices=["pretrain", "sft"])
    parser.add_argument("--pad_with_eos", action="store_true")
    args_cli = parser.parse_args()

    # 1. Config íŒŒì¼ ë¡œë“œ
    with open(args_cli.config_file, "r") as f:
        config = yaml.safe_load(f)

    # 2. Tokenizer & Model ì´ˆê¸°í™”
    # (ì´ë¯¸ Hubì— ì˜¬ë¦° ëª¨ë¸ì´ ìˆë‹¤ë©´ AutoModel.from_pretrained("YourID/...")ë¡œ ë¡œë“œ ê°€ëŠ¥)
    # ì—¬ê¸°ì„œëŠ” Config ê¸°ë°˜ ì´ˆê¸°í™” ì˜ˆì‹œì…ë‹ˆë‹¤.
    tokenizer = AutoTokenizer.from_pretrained(config['pretrained_model_name'])
    
    # [ì¤‘ìš”] SFT ëª¨ë“œì¼ ê²½ìš° Chat Template ì„¤ì • (í•„ìš”ì‹œ)
    if args_cli.mode == "sft":
        # ... (ì´ì „ ëŒ€í™”ì˜ Chat Template ì„¤ì • ì½”ë“œ ì¶”ê°€) ...
        pass

    llada_config = LladaConfig(
        backbone_model_name=config['pretrained_model_name'],
        mask_token_id=tokenizer.mask_token_id
    )
    
    model = KoMiniLlada(llada_config)
    
    # Special Tokenì´ ì¶”ê°€ë˜ì—ˆë‹¤ë©´ ì„ë² ë”© ë¦¬ì‚¬ì´ì¦ˆ
    model.resize_token_embeddings(len(tokenizer))

    # 3. ë°ì´í„°ì…‹ ì¤€ë¹„
    # ê¸°ì¡´ prepare_dataset í•¨ìˆ˜ í™œìš©
    full_dataset = prepare_dataset(
        tokenizer, 
        dataset_config=config['dataset_config']['pre_training' if args_cli.mode == 'pretrain' else 'fine_tuning']['dataset_list'], 
        max_seq_len=config['max_seq_len'],
        mode=args_cli.mode,
        pad_with_eos=args_cli.pad_with_eos
    )

    # [í•µì‹¬ 1] Train/Eval Split
    # Configì— ìˆëŠ” test_size ì‚¬ìš©
    test_size = config['dataset_config'].get('pre_training' if args_cli.mode == 'pretrain' else 'fine_tuning').get('test_size', 0.01)
    split_datasets = full_dataset.train_test_split(test_size=test_size, seed=config.get('random_seed', 42))
    
    train_dataset = split_datasets['train']
    eval_dataset = split_datasets['test']

    # [í•µì‹¬ 2] 'labels' ì»¬ëŸ¼ ìƒì„±
    # HF TrainerëŠ” ë°ì´í„°ì…‹ì— 'labels'ê°€ ìˆì–´ì•¼ í•™ìŠµ ëª¨ë“œ(Loss ê³„ì‚°)ë¡œ ì§„ì…í•©ë‹ˆë‹¤.
    # KoMiniLladaëŠ” input_idsë¥¼ ë³µì‚¬í•´ì„œ labelsë¡œ ì£¼ë©´ ë‚´ë¶€ì—ì„œ ë§ˆìŠ¤í‚¹í•˜ê³  Lossë¥¼ ê³„ì‚°í•©ë‹ˆë‹¤.
    print("Mapping labels...")
    train_dataset = train_dataset.map(lambda x: {'labels': x['input_ids']})
    eval_dataset = eval_dataset.map(lambda x: {'labels': x['input_ids']})

    # 4. TrainingArguments ì„¤ì • (config.yamlì˜ ë‚´ìš© ë§¤í•‘)
    train_conf = config['train_config']
    
    training_args = TrainingArguments(
        output_dir=args_cli.output_dir,
        overwrite_output_dir=True,
        
        # í•™ìŠµ íŒŒë¼ë¯¸í„°
        num_train_epochs=train_conf.get('num_epochs', 3),
        per_device_train_batch_size=train_conf.get('batch_size', 8),
        per_device_eval_batch_size=train_conf.get('batch_size', 8),
        gradient_accumulation_steps=train_conf.get('gradient_accumulation_steps', 1),
        learning_rate=float(train_conf.get('learning_rate', 5e-5)),
        weight_decay=0.01,
        
        # í‰ê°€ ë° ì €ì¥ ì „ëµ
        evaluation_strategy="steps",
        eval_steps=train_conf.get('eval_steps', 1000),
        save_strategy="steps",
        save_steps=train_conf.get('eval_steps', 1000),
        save_total_limit=2, # ì²´í¬í¬ì¸íŠ¸ ê°œìˆ˜ ì œí•œ (ìš©ëŸ‰ ê´€ë¦¬)
        load_best_model_at_end=True,
        metric_for_best_model="loss",
        
        # í•˜ë“œì›¨ì–´ ë° íš¨ìœ¨ì„±
        fp16=torch.cuda.is_available(), # GPU ìˆìœ¼ë©´ fp16 ìë™ ì‚¬ìš©
        dataloader_num_workers=train_conf.get('num_workers', 4),
        
        # [ì¤‘ìš”] ì»¤ìŠ¤í…€ ëª¨ë¸ ì‚¬ìš© ì‹œ í•„ìˆ˜ ì˜µì…˜
        # Trainerê°€ ì•Œì§€ ëª»í•˜ëŠ” ì»¬ëŸ¼(labels ë“±)ì„ ìë™ìœ¼ë¡œ ì§€ìš°ì§€ ì•Šë„ë¡ ì„¤ì •
        remove_unused_columns=False, 
        
        # ë¡œê¹…
        logging_steps=100,
        report_to="none", # wandb ë“±ì„ ì“´ë‹¤ë©´ "wandb"
        run_name="mini-llada-run"
    )

    # 5. Trainer ì´ˆê¸°í™”
    trainer = Trainer(
        model=model,
        args=training_args,
        train_dataset=train_dataset,
        eval_dataset=eval_dataset,
        data_collator=DataCollatorWithPadding(tokenizer), # Dynamic Padding ì§€ì›
        callbacks=[GenerateSampleCallback(tokenizer)] # (ì„ íƒì‚¬í•­) ìƒì„± ê²°ê³¼ í™•ì¸ìš©
    )

    # 6. í•™ìŠµ ì‹œì‘
    print("ğŸš€ Start Training...")
    trainer.train()

    # 7. ìµœì¢… ëª¨ë¸ ì €ì¥
    print(f"ğŸ’¾ Saving final model to {args_cli.output_dir}/final")
    trainer.save_model(os.path.join(args_cli.output_dir, "final"))
    tokenizer.save_pretrained(os.path.join(args_cli.output_dir, "final"))

if __name__ == "__main__":
    main()