import torch
from torch.utils.data import Dataset
from transformers import AutoTokenizer
from datasets import load_dataset

class PreTokenizedDataset(Dataset):
    def __init__(self, input_ids_list):
        self.input_ids = torch.tensor(input_ids_list, dtype=torch.long)

    def __len__(self):
        return len(self.input_ids)

    def __getitem__(self, idx):
        return self.input_ids[idx]

def get_tokenizer(model_name="EleutherAI/polyglot-ko-1.3b"):
    tokenizer = AutoTokenizer.from_pretrained(model_name)
    
    if tokenizer.mask_token is None:
        tokenizer.add_special_tokens({'mask_token': '[MASK]'})
    
    if tokenizer.pad_token is None:
        tokenizer.pad_token = tokenizer.eos_token
        
    return tokenizer

def prepare_data(tokenizer, max_seq_len=512, dataset_size=None):
    print(f"â³ ë°ì´í„° ë¡œë“œ ë° ì „ì²˜ë¦¬ ì¤‘... (Max Len: {max_seq_len})")
    
    all_texts = []

    # 1. ìœ„í‚¤ë°±ê³¼ (ì§€ì‹ í•™ìŠµìš©) - ì»¬ëŸ¼ëª…: 'text' ë“±
    # ë°ì´í„°ê°€ ë§ìœ¼ë‹ˆ ì¼ë¶€ë§Œ ë¡œë“œ (ì˜ˆ: 10,000ê°œ)
    print("   - ìœ„í‚¤ë°±ê³¼ ë¡œë“œ ì¤‘...")
    wiki_data = load_dataset("heegyu/kowiki-paragraphs", split="train") 
    all_texts.extend([item['text'] for item in wiki_data])

    # 2. êµê³¼ì„œ (ë¬¸ë²•/ìƒì‹ í•™ìŠµìš©) - ì»¬ëŸ¼ëª…: 'text'
    print("   - êµê³¼ì„œ ë°ì´í„° ë¡œë“œ ì¤‘...")
    textbook_data = load_dataset("maywell/korean_textbooks", split="train")
    all_texts.extend([item['text'] for item in textbook_data])

    # 3. KoAlpaca (ì§€ì‹œ ìˆ˜í–‰ í•™ìŠµìš©) - ì»¬ëŸ¼ëª…: 'instruction', 'output'
    print("   - KoAlpaca ë¡œë“œ ì¤‘...")
    alpaca_data = load_dataset("beomi/KoAlpaca-v1.1a", split="train")
    # AlpacaëŠ” ì§ˆë¬¸-ë‹µë³€ í¬ë§·ìœ¼ë¡œ ë³€í™˜í•´ì„œ ì¶”ê°€
    alpaca_texts = [f"ì§ˆë¬¸: {item['instruction']} ë‹µë³€: {item['output']}" for item in alpaca_data]
    all_texts.extend(alpaca_texts)
    
    print(f"ğŸ“Š ì´ ë°ì´í„° ê°œìˆ˜: {len(all_texts)}ê°œ")
    
    # 4. í† í°í™” (í•œ ë²ˆì— ì²˜ë¦¬)
    print("â³ í†µí•© ë°ì´í„° í† í°í™” ì§„í–‰ ì¤‘...")
    encodings = tokenizer(
        all_texts, 
        max_length=max_seq_len, 
        padding="max_length", 
        truncation=True, 
        return_tensors="pt"
    )
    
    # ë©”ëª¨ë¦¬ íš¨ìœ¨ì„ ìœ„í•´ ë¦¬ìŠ¤íŠ¸ë¡œ ë³€í™˜
    input_ids_list = [ids.tolist() for ids in encodings["input_ids"]]
    
    return PreTokenizedDataset(input_ids_list)