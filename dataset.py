import torch
from torch.utils.data import Dataset
from transformers import AutoTokenizer
from datasets import load_dataset

class PreTokenizedDataset(Dataset):
    def __init__(self, input_ids_list):
        self.input_ids = torch.tensor(input_ids_list, dtype=torch.long)

    def __len__(self):
        return len(self.input_ids)

    def __getitem__(self, idx):
        return self.input_ids[idx]

def get_tokenizer(model_name="EleutherAI/polyglot-ko-1.3b"):
    tokenizer = AutoTokenizer.from_pretrained(model_name)
    
    if tokenizer.mask_token is None:
        tokenizer.add_special_tokens({'mask_token': '[MASK]'})
    
    if tokenizer.pad_token is None:
        tokenizer.pad_token = tokenizer.eos_token
        
    return tokenizer

def prepare_data(tokenizer, max_seq_len=512):
    print("â³ ë°ì´í„°ì…‹ ë¡œë“œ ë° ì²˜ë¦¬ ì¤‘...")
    
    # 1. ë°ì´í„°ì…‹ë“¤ì„ ë¡œë“œí•˜ê³  í•©ì¹©ë‹ˆë‹¤.
    wiki_data = load_dataset("maywell/ko_wikidata_QA", split="train[:10000]")
    gpt_data = load_dataset("maywell/ko-gpt3_14k", split="train[:10000]")
    alpaca_data = load_dataset("beomi/KoAlpaca-v1.1a", split="train[:10000]")
    
    # 2. í¬ë§·íŒ… í•¨ìˆ˜ (ì œë„ˆë ˆì´í„°ë¡œ ì²˜ë¦¬í•´ì„œ ë©”ëª¨ë¦¬ ì•„ë‚Œ)
    def format_wiki(example): return {'text': f"ì§ˆë¬¸: {example['instruction']} ë‹µë³€: {example['output']}"}
    def format_gpt(example): return {'text': f"ì§ˆë¬¸: {example['question']} ë‹µë³€: {example['answer']}"}
    def format_alpaca(example): return {'text': f"ì§ˆë¬¸: {example['instruction']} ë‹µë³€: {example['output']}"}
    
    wiki_data = wiki_data.map(format_wiki, remove_columns=wiki_data.column_names)
    gpt_data = gpt_data.map(format_gpt, remove_columns=gpt_data.column_names)
    alpaca_data = alpaca_data.map(format_alpaca, remove_columns=alpaca_data.column_names)
    
    # í•©ì¹˜ê¸°
    from datasets import concatenate_datasets
    dataset = concatenate_datasets([wiki_data, gpt_data, alpaca_data])
    
    # 3. í† í¬ë‚˜ì´ì§• (ë°°ì¹˜ ë‹¨ìœ„ë¡œ ì²˜ë¦¬í•´ì„œ ë©”ëª¨ë¦¬ í„°ì§ ë°©ì§€)
    def tokenize_function(examples):
        return tokenizer(
            examples["text"], 
            padding="max_length", 
            truncation=True, 
            max_length=max_seq_len,
            return_tensors="pt"
        )
    
    # batched=Trueë¡œ í•˜ë©´ ì•Œì•„ì„œ ë‚˜ëˆ ì„œ ì²˜ë¦¬í•¨
    tokenized_dataset = dataset.map(tokenize_function, batched=True, remove_columns=["text"])
    
    # PyTorch í…ì„œë¡œ í¬ë§· ì„¤ì •
    tokenized_dataset.set_format("torch")
    
    return tokenized_dataset

# def prepare_data(tokenizer, max_seq_len=512, dataset_size=None):
#     print(f"â³ ë°ì´í„° ë¡œë“œ ë° ì „ì²˜ë¦¬ ì¤‘... (Max Len: {max_seq_len})")
    
#     all_texts = []

#     # 1. ìœ„í‚¤ë°±ê³¼ (ì§€ì‹ í•™ìŠµìš©) - ì»¬ëŸ¼ëª…: 'text' ë“±
#     # ë°ì´í„°ê°€ ë§ìœ¼ë‹ˆ ì¼ë¶€ë§Œ ë¡œë“œ (ì˜ˆ: 10,000ê°œ)
#     print("   - ìœ„í‚¤ë°±ê³¼ ë¡œë“œ ì¤‘...")
#     wiki_data = load_dataset("maywell/ko_wikidata_QA", split="train")
#     wiki_texts = [f"ì§ˆë¬¸: {item['instruction']} ë‹µë³€: {item['output']}" for item in wiki_data] 
#     all_texts.extend(wiki_texts)

#     # 2. Ko - ì»¬ëŸ¼ëª…: 'text'
#     print("   - êµê³¼ì„œ ë°ì´í„° ë¡œë“œ ì¤‘...")
#     gpt_data = load_dataset("maywell/ko-gpt3_14k", split="train")
#     gpt_texts = [f"ì§ˆë¬¸: {item['question']} ë‹µë³€: {item['answer']}" for item in gpt_data]
#     all_texts.extend(gpt_texts)

#     # 3. KoAlpaca (ì§€ì‹œ ìˆ˜í–‰ í•™ìŠµìš©) - ì»¬ëŸ¼ëª…: 'instruction', 'output'
#     print("   - KoAlpaca ë¡œë“œ ì¤‘...")
#     alpaca_data = load_dataset("beomi/KoAlpaca-v1.1a", split="train")
#     # AlpacaëŠ” ì§ˆë¬¸-ë‹µë³€ í¬ë§·ìœ¼ë¡œ ë³€í™˜í•´ì„œ ì¶”ê°€
#     alpaca_texts = [f"ì§ˆë¬¸: {item['instruction']} ë‹µë³€: {item['output']}" for item in alpaca_data]
#     all_texts.extend(alpaca_texts)
    
#     print(f"ğŸ“Š ì´ ë°ì´í„° ê°œìˆ˜: {len(all_texts)}ê°œ")
    
#     # 4. í† í°í™” (í•œ ë²ˆì— ì²˜ë¦¬)
#     print("â³ í†µí•© ë°ì´í„° í† í°í™” ì§„í–‰ ì¤‘...")
#     encodings = tokenizer(
#         all_texts, 
#         max_length=max_seq_len, 
#         padding="max_length", 
#         truncation=True, 
#         return_tensors="pt"
#     )
    
#     # ë©”ëª¨ë¦¬ íš¨ìœ¨ì„ ìœ„í•´ ë¦¬ìŠ¤íŠ¸ë¡œ ë³€í™˜
#     input_ids_list = [ids.tolist() for ids in encodings["input_ids"]]
    
#     return PreTokenizedDataset(input_ids_list)