model_config:
  dim: 2048
  depth: 18 
  head: 16
  intermediate_size: 5632
  max_seq_len: 2048

tokenizer_config:
  vocab_size: 52000

dataset_config:
  pretrain:
    test_size: 0.001
  sft:
    test_size: 0.001

train_config:
  eval_steps: 10000
  batch_size: 8
  num_workers: 8
  learning_rate: 0.0001
  num_epochs: 1
  gradient_accumulation_steps: 16
  warmup_steps: 2000
  max_grad_norm: 1.0
  weight_decay: 0.1
  logging_steps: 100
  save_total_limit: 2
  fp16: False
  bf16: True
  gradient_checkpointing: True
  report_to: "tensorboard"
  deepspeed: "configs/ds_config.json"